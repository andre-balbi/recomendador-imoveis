{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8f15b4-2a70-418f-9c45-44645db34d16",
   "metadata": {},
   "source": [
    "# Justificativa para o modelo de classifica√ßao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b304c-a902-4519-a1ab-ca4bcd86c803",
   "metadata": {},
   "source": [
    "-Beleza! O recomendador parece estar funcionar, mas como posso saber se ele est√° funcionando bem? A avalia√ß√£o de um sistema de recomenda√ß√£o pode ser bastante complicada. A m√©trica de ouro para um sistema de recomenda√ß√£o √© o quanto o sistema agregar√° valor ao usu√°rio e ao neg√≥cio. Por fim, podemos realizar um teste A / B para ver se as recomenda√ß√µes ajudaram o usu√°rio a encontrar o im√≥vel de uma forma mais flu√≠da se comparado √†s t√©cnicas tradicionais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313bb74-ca75-4add-a7ad-a4999295a1b8",
   "metadata": {},
   "source": [
    "Mas existem outras t√©cnicas para avaliar um sistema de recomenda√ß√£o, ou seja, distinguir as recomenda√ß√µes boas das ruins. No caso bin√°rio, isso √© feito indicando ‚Äú1‚Äù como sendo uma boa recomenda√ß√£o, enquanto ‚Äú0‚Äù significa uma recomenda√ß√£o ruim. Ap√≥s realizada suficiente intera√ß√£o do usu√°rio com o sistema de recomenda√ß√£o, e de posse dos dados com seus respectivos labels (target, ou seja, ‚Äú0‚Äù ou ‚Äù1‚Äù),  podemos treinar um modelo de classifica√ß√£o nos dados que possuem label e testar nos dados que n√£o possuem label, retornando um ranking (predict proba) com os an√∫ncios que mais se aproximam daqueles classificados como ‚Äú1‚Äù. Assim, ao mesmo tempo que podemos avaliar se o sistema est√° funcionando como previsto, conseguimos realizar uma esp√©cie de ‚Äúcuradoria‚Äù nos an√∫ncios que o usu√°rio provavelmente n√£o veria por estar em clusters diferentes e/ou por conta do enorme volume de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef38a574-ff34-4f70-9ff4-2f34ae64f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "from random import shuffle\n",
    "import webbrowser\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import category_encoders as ce # Target encoder\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from skopt import dummy_minimize, gp_minimize, forest_minimize\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk \n",
    "from nltk import tokenize\n",
    "from string import punctuation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "sns.set()\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c81ad0b-0778-434f-8471-6a1d7aaae56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/04_feat_eng_df/'\n",
    "\n",
    "# Nome dos arquivos dentro do diretorio acima (raw_df)\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "# Dataframe dos dados sem tratamento\n",
    "df = pd.read_csv(path + filenames[-1], sep='|') # -1 para pegar o ultimo dataframe adquirido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e87f6caa-af0b-4d4e-8328-086f4bfb3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando colunas antes do active learning\n",
    "# cols = ['list_id', 'title', 'logradouro', 'latitude', \\\n",
    "#         'longitude', 'zipcode', 'source_x', 'desc_full', 'altitude', 'sellername', 'state', 'region', 'complemento']\n",
    "\n",
    "df_mod = df.drop('desc_full', axis=1) # Dataframe modificado\n",
    "desc = df['desc_full'] # Series com a descricao de cada ap para o tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d4b2c1-2aa7-4108-9ce2-47c1bfc70339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['list_id', 'link', 'price', 'condominio', 'rooms', 'bathrooms',\n",
       "       'garage_spaces', 'size', 'pictures', 'desc_full', 'padrao_bairro',\n",
       "       'price_total', 'barra funda', 'bela vista', 'belenzinho', 'bom retiro',\n",
       "       'cambuci', 'campos eliseos', 'centro', 'consolacao', 'liberdade', 'luz',\n",
       "       'morro dos ingleses', 'parque industrial tomas edson', 'republica',\n",
       "       'santa cecilia', 'santa efigenia', 'se', 'varzea da barra funda',\n",
       "       'vila buarque', 'vila deodoro'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cad85-2b56-4aaa-bb1d-f192a8df2b4d",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776ae134-3171-4bb5-b100-7109ca53c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['list_id', 'link', 'price', 'condominio', 'rooms', 'bathrooms',\n",
       "       'garage_spaces', 'size', 'pictures', 'padrao_bairro', 'price_total',\n",
       "       'barra funda', 'bela vista', 'belenzinho', 'bom retiro', 'cambuci',\n",
       "       'campos eliseos', 'centro', 'consolacao', 'liberdade', 'luz',\n",
       "       'morro dos ingleses', 'parque industrial tomas edson', 'republica',\n",
       "       'santa cecilia', 'santa efigenia', 'se', 'varzea da barra funda',\n",
       "       'vila buarque', 'vila deodoro'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mod.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16db8be-3de6-44d2-aa2e-6a93ea44e3f4",
   "metadata": {},
   "source": [
    "## Modelo Naive para Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e65b4c1-e03a-42fd-bbe7-251ec6d8f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labeling = RandomForestClassifier(n_estimators=200, \n",
    "                                  random_state=0,\n",
    "                                  min_samples_leaf=1,\n",
    "                                  class_weight='balanced', \n",
    "                                  n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5053e24-ce40-4112-81c1-c7e7a19eb178",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Funcoes para a otimiza√ß√£o bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb215d7-0d85-42ae-a1bf-11679f87fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_rf(params):\n",
    "    \"\"\" Fun√ß√£o para tunar o modelo do random forest\"\"\"\n",
    "    bootstrap = params[0]\n",
    "    max_depth = params[1]\n",
    "    max_features = params[2] \n",
    "    min_samples_leaf = params[3] \n",
    "    min_samples_split = params[4]\n",
    "    n_estimators = params[5]\n",
    "    \n",
    "    model = RandomForestClassifier(bootstrap=bootstrap, max_depth=max_depth, max_features=max_features,\n",
    "                          min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_estimators=n_estimators)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict_proba(X)[: ,1]  # y_pred entre treino e teste\n",
    "      \n",
    "    return -average_precision_score(y, y_pred)\n",
    "\n",
    "space_rf = [[True, False],\n",
    "            [1, 2, 5, 10, None],\n",
    "            ['auto', 'sqrt'], \n",
    "            (1, 4), \n",
    "            (2, 10),\n",
    "            (500, 1000)]\n",
    "##################################################################################################################\n",
    "\n",
    "def tuning_lr(params):\n",
    "    \"\"\" Fun√ß√£o para tunar o modelo logistic regression \"\"\"\n",
    "    solver = params[0]\n",
    "    penalty = params[1]\n",
    "    C = params[2]\n",
    "    \n",
    "    model = LogisticRegression(solver=solver, penalty=penalty, C=C)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict_proba(X)[: ,1]  # y_pred entre treino e teste\n",
    "    \n",
    "    return -average_precision_score(y, y_pred)\n",
    "\n",
    "\n",
    "space_lr = [['newton-cg', 'lbfgs', 'liblinear'],\n",
    "            ['l2'],\n",
    "            (0.01, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6ce8f-9439-4bd3-a905-448f1723dce9",
   "metadata": {},
   "source": [
    "## fun√ß√£o para o Active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31fa77ce-a07a-4e5a-8776-d1f64c1215d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(df):\n",
    "    \"\"\" return to_label, top_itens\"\"\"\n",
    "\n",
    "    df = df.drop(['link'], axis=1) # Delerando o link antes de treinar o modelo\n",
    "    df_model = df[df['label'].notnull()] # Selecionando os dados com label para treinar o modelo\n",
    "    \n",
    "    # Se houver mais de 10 itens com label &\n",
    "    # Se todos os itens nao forem iguais a 0, treinar o modelo\n",
    "    if (len(df_model)>= 10):\n",
    "        #(0 is not df_model.value_counts().to_list()):\n",
    "        #(len(df_model[df_model['label'] == 1] >= 2)): \n",
    "           \n",
    "        X = df_model.drop('label', axis=1) # Features\n",
    "        y = df_model['label'].astype(int) # Target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=0) # Split entre treino e teste\n",
    "        \n",
    "        print('========== Informa√ß√µes do treinamento ==========')\n",
    "        print(f'{round(X_train.shape[0] / df.shape[0], 2)*100}% dos dados anotados, sendo que {round(y_train.mean(), 2) * 100}% desses dados sao positivos')\n",
    "        \n",
    "        model_labeling.fit(X_train, y_train) # Fitando entre treino e teste (pipeline para labeling)\n",
    "        y_pred = model_labeling.predict_proba(X_test)[: ,1]  # y_pred entre treino e teste (pipeline para laberling)\n",
    "        try:\n",
    "            print(f'average precision score: {average_precision_score(y_test, y_pred)}') # Precision\n",
    "            print(f'roc auc score: {roc_auc_score(y_test, y_pred)}') # ROC\n",
    "        except:\n",
    "            print('Only one class present in y_true. ROC AUC score is not defined in that case.')\n",
    "        \n",
    "        ################################# Selecao dos itens para label ##########################################\n",
    "        \n",
    "        df_unlabeled = df[df['label'].isnull()] # Data sem label\n",
    "        X_unlabeled = df_unlabeled.drop('label', axis=1) # X sem label (dropando a feature label)\n",
    "        \n",
    "        y_pred_unlabeled = model_labeling.predict_proba(X_unlabeled)[:, 1] # y_pred para os dados sem label\n",
    "        df_unlabeled['y_pred'] = y_pred_unlabeled # Juntando a o y_pred sem label com o dataframe sem label\n",
    "        \n",
    "        top_itens = df_unlabeled.sort_values(by='y_pred', ascending=False)[:20].index # Top 20 dos dados do dataframe sem label\n",
    "        to_label_undecided = df_unlabeled[(df_unlabeled['y_pred'] >= 0.50) & (df_unlabeled['y_pred'] < 1)] # Item para labeling (em torno de 50%)\n",
    "\n",
    "        if len(to_label_undecided) < 7: # Se Houver dados insuficientes para gerar pelo menos 7 valores em torno de 0.4 e 0.6\n",
    "            to_label = list(df_unlabeled.sample(5).index) # Adquire 5 valores aleatorios\n",
    "        else:\n",
    "            to_label = list(pd.concat([to_label_undecided.sample(7),  # Adquiri 7 valores indecisos\n",
    "                                       df_unlabeled.sample(3)]).index) # Adquiri 3 valores aleatorios\n",
    "\n",
    "        print('\\n========= y_pred do item para treino ==========')\n",
    "        print(df_unlabeled.loc[to_label]['y_pred']) # y_pred do dado sorteado\n",
    "        \n",
    "        print('\\n========= Feature Importance ==========') # Feature Importance\n",
    "        feat_importance = pd.DataFrame(zip(X_train.columns, model_labeling.feature_importances_), columns=['feature', 'importance'])\n",
    "        feat_importance = feat_importance.set_index('feature').sort_values('importance', ascending=False)\n",
    "        print(feat_importance[:5]) # 5 features mais importantes\n",
    "       \n",
    "    else: # Se houver menos de 10 itens com label\n",
    "        print('Ps: Itens Aleat√≥rios')\n",
    "        to_label = list(df.sample(10).index) # Sorteia numeros aleat√≥rio entre os indices sem label\n",
    "        top_itens = to_label\n",
    "        \n",
    "    shuffle(to_label) # Embaralha os itens para treino\n",
    "    \n",
    "    return to_label, top_itens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3f59a-db81-4559-bbf6-c82f6d99255f",
   "metadata": {},
   "source": [
    "## Fun√ß√£o para o labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0119e86a-7bdf-4669-9f51-07d70160c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df, print_recommendation=False):\n",
    "    \"\"\" Funcao para inserir o label 0 e 1 no dataframe \"\"\"\n",
    "    \n",
    "    df_temp = df.copy()# Lista para guardar os indices dos itens\n",
    "    if {'label'}.issubset(df_temp.columns) == False: # Verifica se h√° a coluna 'label'\n",
    "        df_temp['label'] = np.nan # Se nao hover cria uma coluna 'label'\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        to_label, top_itens = active_learning(df_temp) # Adquirindo o dados para o active learning e os top itens\n",
    "        \n",
    "        ##############################################################################        \n",
    "        # Printa os X melhores predic probas adquiridos pela funcao 'active_learning'\n",
    "        if print_recommendation==True:\n",
    "            print('\\n  ========== Recomenda√ß√£o Inicial ==========')\n",
    "            for i in top_itens:\n",
    "                \tprint(df_temp.loc[i]['link'])\n",
    "        ###############################################################################\n",
    "\n",
    "        for n in to_label: # Looping para os itens selecionados para labeling\n",
    "            \n",
    "            ape = df_temp.loc[n] # Seleciona ape sorteado\n",
    "            \n",
    "            print('\\n ========== Dados do item ======== \\n', ape['link']) # Imprime o link\n",
    "            print(ape, '\\n') # Imprime as caracteristicas dropando as colunas que contem apenas zero geradas pelo\n",
    "            print('='*80)\n",
    "            webbrowser.open(ape['link']) # Abre o link\n",
    "            \n",
    "            n_labels = df_temp[~df_temp['label'].isnull()].shape[0] # Quantidade de itens com label\n",
    "            print(f'Quantidade de itens com label: {n_labels}') \n",
    "            \n",
    "            ## 1a condicional (0 ou 1)\n",
    "            while True:\n",
    "                inp = str(input('Label: 0 ou 1')) # Adquire o label do items selecionado\n",
    "                \n",
    "                if (inp == '0') | (inp == '1'): # Verifica se o item √© 0 ou 1\n",
    "                    df_temp['label'].loc[int(n)] = inp # Insere o label no item selecionado\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            df_temp.to_csv(f'./data/05_labeled_df/df_w_label.csv', sep='|',  index=False) # Salva o dataframe\n",
    "            \n",
    "            ## 2a condicional (continuar? s ou n)\n",
    "            while True:\n",
    "                inp = str(input('Continuar? S ou N')).lower() # Le a resposta\n",
    "                if (inp == 's') | (inp == 'n'): # Se a resposta for 'sim' ou 'n√£o'\n",
    "                    if inp == 'n': # Condicional para saber se nao continuoa com o lopping\n",
    "                        n_labels = df_temp[~df_temp['label'].isnull()].shape[0] # Quantidade de labels no dataframe\n",
    "                        print(f'Quantidade de itens com label: {n_labels}') # Printa a quantidade de itens com label\n",
    "\n",
    "                        return df_temp # Retorna o dataframe + label\n",
    "\n",
    "                    else:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098eecb8-1773-49df-bf87-48fe1fe3cfc6",
   "metadata": {},
   "source": [
    "## Fun√ß√£o de Preprocessamento/Vetoriza√ß√£o de texto/Clusteriza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f257196-ce34-4653-ba28-711ac426a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def nlp_trat(dataset):\n",
    "    \"\"\" Fun√ßao para a criacao da matriz espar√ßa do tfidf\"\"\"\n",
    "    tfidf = TfidfVectorizer(lowercase=True, max_features=50, min_df=2, ngram_range=(1, 2))\n",
    "    token_espaco = tokenize.WhitespaceTokenizer()\n",
    "    palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "    \n",
    "    stemmer = nltk.RSLPStemmer()\n",
    "    token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "    \n",
    "    # Cria√ß√£o de lista com pontua√ß√£o\n",
    "    pontuacao = list()\n",
    "    for ponto in punctuation:\n",
    "        pontuacao.append(ponto)\n",
    "    pontuacao\n",
    "    \n",
    "    # Cria√ß√£o de lista complementar de palavras para remover do dataset (justificativas no notebook de storytelling)\n",
    "    compl_stop_words = ['dormitorio','sao','paulo','apartamento','apto','sp','r$','m2','alugar','aluga','dormitorios',\n",
    "                        'aluguel','locacao','para','com','em','por','mes','r', 'codigo', 'anuncio']\n",
    "    \n",
    "    # Lista final de palavras e caracteres para serem removiodos\n",
    "    pontuacao_stop_words = pontuacao + palavras_irrelevantes + compl_stop_words\n",
    "    frase_processada = list()\n",
    "    nova_frase = list()\n",
    "    \n",
    "\n",
    "    \n",
    "    for desc in dataset:\n",
    "        palavras_texto = token_pontuacao.tokenize(desc)\n",
    "        for palavra in palavras_texto:\n",
    "            if palavra not in pontuacao_stop_words and palavra.isalpha():\n",
    "                nova_frase.append(stemmer.stem(palavra))\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    frase_final = frase_processada\n",
    "    \n",
    "# TFIDF\n",
    "    tfidf_bruto = tfidf.fit_transform(frase_final)\n",
    "    \n",
    "    return tfidf_bruto\n",
    "\n",
    "sparsed_df = nlp_trat(desc) # TF-IDF da descricao completa\n",
    "scipy.sparse.save_npz('./tmp/sparsed_df.npz', sparsed_df) # Salvando a matrix espar;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90471a3d-a810-498d-bd9a-177e09e39acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2110x50 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 105248 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.sparse.load_npz('./tmp/sparsed_df.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88957f6-dc15-4628-8aa4-c31b26ea559b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588fd028-617e-4c44-bfd5-226b89145f35",
   "metadata": {},
   "source": [
    "# Rodando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfdcc2-f872-4bd9-928e-cc7877717f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fun√ß√£o para labeling\n",
    "df_mod = get_label(df_mod, print_recommendation=True)\n",
    "\n",
    "# Preparando os dados com o dataframe labeled\n",
    "df_labeled = df_mod[df_mod['label'].notnull()] # Selecionando os dados com label para treinar o modelo\n",
    "X_numeric = df_labeled.drop(['label', 'link'], axis=1) # X para tunnin com dados numericos\n",
    "X_tfidf = sparsed_df.tocsr()[df_labeled.index] # X para tunning com tfidf\n",
    "print(desc.loc[df_labeled.index])\n",
    "y = df_labeled['label'].astype(int) # Target\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=0) # Split entre treino e teste\n",
    "\n",
    "# Fazendo o predict do modelo com os dados sem label\n",
    "print('\\n Aguarde um momento .......... (vetoriza√ß√£o das vari√°veis categ√≥ricas)\\n') # O treino pode levar algums minutos, dependendo do tamanho do dataframe\n",
    "df_unlabeled = df_mod[df_mod['label'].isnull()] # dataframe sem label\n",
    "X_unlabeled_numeric = df_unlabeled.drop(['link', 'label'], axis=1) # Features sem label para treino dados numericos\n",
    "X_unlabeled_tfidf= sparsed_df.tocsr()[df_unlabeled.index] # Features sem label Fazendo a vetorizacao dos dados desc\n",
    "\n",
    "for i in zip([X_numeric, X_tfidf], [X_unlabeled_numeric, X_unlabeled_tfidf], ['Recomendacao final', 'Tente dar uma olhada nestes tb üôÇ']):\n",
    "    \n",
    "    X = i[0]\n",
    "    X_unlabeled =i[1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=0) # Split entre treino e teste\n",
    "    \n",
    "    print('\\n Aguarde um momento .......... (tunando o modelo)\\n') # O treino pode levar algums minutos, dependendo do tamanho do dataframe\n",
    "    \n",
    "    ''' ============== Bayesian Optimization and Hyperparameter Tuning ================ '''\n",
    "    \n",
    "    ''' ================================ RANDOM FOREST ================================ '''\n",
    "    \n",
    "    # Tunando modelo random forest\n",
    "    result = forest_minimize(tuning_rf, space_rf, random_state=0, n_random_starts=20, n_calls=30, verbose=False)\n",
    "    \n",
    "    bootstrap = result.x[0]\n",
    "    max_depth = result.x[1]\n",
    "    max_features = result.x[2]\n",
    "    min_samples_leaf = result.x[3] \n",
    "    min_samples_split = result.x[4] \n",
    "    n_estimators = result.x[5]\n",
    "    \n",
    "    # Modelo Random Forest\n",
    "    rf = RandomForestClassifier(bootstrap=bootstrap, max_depth=max_depth, max_features=max_features,\n",
    "                                min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_estimators=n_estimators,\n",
    "                                class_weight='balanced', random_state=0, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    ''' ================================ LOGISTIC REGRESSION ================================ '''\n",
    "        \n",
    "     # Tunando modelo Logistic Regression\n",
    "    result = forest_minimize(tuning_lr, space_lr, random_state=0, n_random_starts=20, n_calls=30, verbose=False)\n",
    "    \n",
    "    solver = result.x[0]\n",
    "    penalty = result.x[1]\n",
    "    C = result.x[2]\n",
    "    \n",
    "    # Modelo Logistic Regression\n",
    "    lr = LogisticRegression(solver=solver, penalty=penalty, C=C)\n",
    "\n",
    "    ''' ================================ ENSEMBLE VOTING ================================ '''\n",
    "    \n",
    "    # instancia√ß√£o do Ensemble\n",
    "    voting_model = VotingClassifier(estimators = [('random forest', rf),\n",
    "                                                  ('logistic regression', lr)],\n",
    "                                                voting = 'soft')\n",
    "    \n",
    "    print('\\n Aguarde um momento .......... (treinando o modelo)\\n') # O treino pode levar algums minutos, dependendo do tamanho do dataframe\n",
    "    \n",
    "    voting_model.fit(X_train, y_train)\n",
    "    y_pred = voting_model.predict_proba(X_test)[: ,1]  # y_pred entre treino e teste\n",
    "    try:\n",
    "        print(f'average precision score: {average_precision_score(y_test, y_pred)}') # Precision\n",
    "        print(f'roc auc score: {roc_auc_score(y_test, y_pred)}') # ROC\n",
    "        \n",
    "    except:\n",
    "        print('Only one class present in y_true. ROC AUC score is not defined in that case.')\n",
    "    \n",
    "    df_unlabeled['y_pred'] = voting_model.predict_proba(X_unlabeled)[:, 1]# Predicts proba para os dados sem label\n",
    "    final_rec_top_itens = df_unlabeled.sort_values('y_pred', ascending=False)[:20].index # indices dos top_itens\n",
    "    \n",
    "    print(f'\\n ========== {i[2]} ==========')\n",
    "    for i in df_mod.loc[final_rec_top_itens]['link']:\n",
    "            print(i)\n",
    "            \n",
    "df_mod.to_csv(f'./data/05_labeled_df/df_w_label.csv', sep='|',  index=False) # Cria o dataframefeat_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e85a6",
   "metadata": {},
   "source": [
    "<img src=\"img/app2.jpg\" width=\"1200\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit ('Anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "809bf3cb40558a57fdf8e1d5c88b4d8645e43317530673dfc5c6aa3a17a141e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
